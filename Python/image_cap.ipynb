{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0fc5173-953a-42e5-b728-bf10dfee3dc1",
   "metadata": {},
   "source": [
    "Give Meaningful Names to Your Photos with AI\n",
    "\n",
    "- BLIP model from Hugging Face's Transformers\n",
    "- Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b0291-ec7b-4a6b-95ac-7c089e4e165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install virtualenv \n",
    "# virtualenv my_env # create a virtual environment my_env\n",
    "# source my_env/bin/activate # activate my_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ec5062-caf0-46c5-a800-36657f3fe575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # installing required libraries in my_env\n",
    "# pip install langchain==0.1.11 gradio==4.44.0 transformers==4.38.2 bs4==0.0.2 requests==2.31.0 torch==2.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293a7a7-232f-4d9c-a985-bb407a458f50",
   "metadata": {},
   "source": [
    "Step 1: Import your required tools from the transformers library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37aa561-1c9e-44b1-8bd3-3bb2fd324364",
   "metadata": {},
   "source": [
    "AutoProcessor : This is a processor class that is used for preprocessing data for the BLIP model. It wraps a BLIP image processor and an OPT/T5 tokenizer into a single processor. This means it can handle both image and text data, preparing it for input into the BLIP model.\n",
    "\n",
    "Note: A tokenizer is a tool in natural language processing that breaks down text into smaller, manageable units (tokens), such as words or phrases, enabling models to analyze and understand the text.\n",
    "\n",
    "BlipForConditionalGeneration : This is a model class that is used for conditional text generation given an image and an optional text prompt. In other words, it can generate text based on an input image and an optional piece of text. This makes it useful for tasks like image captioning or visual question answering, where the model needs to generate text that describes an image or answer a question about an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68eddd97-6e66-45c3-b7ce-447662c2b857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mshimizu/env/btc1_proj1_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mshimizu/env/btc1_proj1_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# Load the pretrained processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b20c83a-0ade-4d34-9540-8ae2868ff87f",
   "metadata": {},
   "source": [
    "Step 2: Fetch the model and initialize a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9168408f-3f44-4ef5-8af6-e4d96e56f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your image, DONT FORGET TO WRITE YOUR IMAGE NAME\n",
    "img_path = \"image.jpg\"\n",
    "# convert it into an RGB format \n",
    "image = Image.open(img_path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "192b4e91-5db4-4b4e-abb4-f0b46bc79c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need a question for image captioning\n",
    "text = \"the image of\"\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc87544-cd8d-42db-863a-8ae38863f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a caption for the image\n",
    "outputs = model.generate(**inputs, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71009cb9-3e41-43ec-8dc3-aa8c4044ed04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image of a lighthouse and the milky\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated tokens to text\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "# Print the caption\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b9c503-b86c-43a2-9a71-f43190bd66eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (btc1_proj1_env)",
   "language": "python",
   "name": "btc1_proj1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
